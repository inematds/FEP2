resuma todo conteudo em topicos

00:00 Alright, and we're back, just got a little haircut, so we're good to go now. Now we're getting into intermediate prompting techniques, so we went through zero shot, additive prompting, multi shot, few shot, all the same thing basically.
00:14 Now we're gonna get into my favorite types. So, chain of thought was my favorite up until reasoning models came out, and the reason why, pun intended, is reasoning models use chain of thought while they're responding.
00:27 So typically before, when we had the old world models, which is 40, 4.5, 3.5, anything Gemini, 2.0 and prior, that's not a thinking model.
00:38 This chain of thought prompting was super helpful if you got one response no matter how good your prompt was because your context windows were small.
00:47 So if you could only put in three paragraphs of context and output a response and you got some hallucination sometimes or you got a copy generated that was, didn't meet your expectations, chain of thought prompting was super helpful to just take that output, feed it as an input to the next prompt, and
01:05 then output a refined version. And like I said before, when the AI or the LM in this case has the opportunity to reflect on its output, this is where it's helpful to be able to, you know, run that prediction again, based on your initial input now, and based on your initial prompt, how do we refine the
01:24 output? In a way you have like two derivatives of information in the prompt now because you had input one, output one, may have input two, which is due x, y, z with output one, and now you have output two taking into consideration what your input one two was, as well as your output one.
01:44 So it has way more to run on to make that prediction for output two. And I'm going to keep saying the word prediction even if it annoys you for me to say it so that drive it home that this is not magic, this is methodical, and this is a series of giving clues and hints to maximize the likelihood of you
02:00 getting the result you want. So in this case, we're going to say that chain of thought prompting is a technique where you prompt the model to produce its reasoning process step by step rather than spitting out the final answer.
02:11 So if traditional output, you put an info, you get out response. The answer is 42, and chain of thought is first, we're going to calculate this.
02:19 then a to consider why. And then, therefore the answer is 42. In essence, basically, we just want to show your work.
02:27 And if you just go into GPT for two seconds, and you go to, let's say, oh, three mini, just because it'll take a very short amount of time, what is the meaning of 42 when it comes to life?
02:41 Okay. And I stand that over. It's going to say reasoning for like two seconds, because it's a very quick reasoning model.
02:47 In this case there you go. If I click on this little plus button, you see this little self-reflection which you'll also see with deep C-gar one which is why the world went crazy because you got a model that could think at the time that was pretty inexpensive to free to run.
03:04 It's basically doing chain of thought through this process. So what used to be manual where let's say in a workflow automation you had one GVT step and then multiple attached to it that it would to take the input of one onto the next, this is a way that you can do that as well.
03:21 And what I see with the say N8N, quite a bit, it's a trend, it's not a trend I necessarily support or advocate for, is people hooking up one agent module to another and saying, oh, I built my own AI agent army that does all my work, all that stuff.
03:38 In production, that sucks. In production, chain prompt in different agents with different sub prompts, with different buffer windows, is really poor engineering design because because this is all prediction you give so many different areas and like you have this surface area of failure or hallucination
03:57 they just keep growing as you add more chain agents so chain prompting is different because you have one prompt and another prompt and another another prompt but all of them are additive meaning you're not going left right center with each and every prompt.
04:14 Ideally, it's like you have context of input one in the context of input two. Yeah, context of input one and two in the context of input three.
04:24 So there is some continuation. There is some consistency there. Whereas with this agent examples that I'm seeing pop up, you have kind of like ADHD agents.
04:34 You have one focus on this, one focus on that. Within some of them, you might chain prompt within that agent and then that output goes to another agent.
04:41 this becomes a mess. So chain prompting is a very good way without raising models even to make things work very sequentially and increase the likelihood that you get what you want.
04:52 So that's chain of thought. Now we have similar to what we said before, few shop prompting, multi-shot. You have few shot COT and when you use the word or the acronym COT to now denote chain of thought, include examples with the reasoning steps in your prompts.
05:10 Question, if John has five apples and needs to go, how many are we? Answer, John starts with five apples, he eats two apples, five minus two equals three, so three are we rooming?
05:21 Zero shot is simply out of prompts to think step by step, and then you just have it all run at once.
05:29 Now with the reasoning models, again, if we go back to here, Oh, three meaning. Typically it's kind of like a zero shot prompt, meaning like a zero shot team of thought prompt.
05:38 It's not going through multiple multiple steps. So I ran in all that reasoning and reflection very quickly and instantaneously, whereas if we take the same thing, okay, copy it and we go to, I'm going to go to O1 pro, which you might not have access to because you can only access it for now on the pro
05:57 account. so let me spell life properly. This should take like two to three business days to actually come up with a response and you'll see it's reasoning has a very long buffering window.
06:11 If I go to details it's now starting step one of thinking. It's then going to evolve to step two eventually of thinking and keep going for possibly up to like 10 to 15 minutes to actually reason through the problem.
06:22 Whereas with O3 Mini there's almost instantaneous kind of a zero shot chain of problem, chain of thought problem for other.
06:29 So it's going to keep doing that. And if we go back here, chain of thought is extremely useful for three main things.
06:36 One, complex reasoning tasks, like I said before, multi-step math problems, logical puzzles in actual business words and business practice. If you have a campaign you're running where you're going from Google sheet leads to qualifying those leads to doing a perplexity check on the quality of those leads
06:54 , where they work, what they do, et cetera, and you want to sequence that part of the process. So you have like some generator AI sprinkled in each part to enrich it, or to enrich the lead or the information, or to do the research using perplexity, and then taking that research from perplexity and then
07:11 refining it in a certain way. This is where chain prompting is useful, and most people don't know they're using chain prompting until you tell them they are because you add in any then, or in make.com, three or four consecutive GPT steps, that is a workflow automation that has chain prompts.
07:30 That's pretty much the architecture. So, and like I said before, chain of thought is baked into region models, so it's good for double checking things, and especially if you want to do like a word count check, and this is more so for the SEO focused folks out there.
07:45 You see 400 words to GP40, you're gonna get like 382 words, or like 387, whatever the flavor of the day is with that model.
07:54 With reasoning models, you have a higher likelihood of hitting that exact number, especially if you use something like 40. You can see here it, if we go to, let's go, the connection failed, let me go to 40, not sure if there's servers or bugging out.
08:11 If I say write me a 10 word sentence, sometimes it can do a good job. So this one's not bad.
08:21 One, two, three, four, five, six, seven, eight, nine, ten. Amazing. Did you tell it? Write a hundred word paragraph, okay?
08:34 This might struggle a bit more. So when I'll just open word counter or it counted on it Take this Paste it Oh 101 words actually not too bad.
08:48 I think they've also recently upgraded for row to be a lot better But if you take something like I can actually click on here.
08:56 It's refresh All right If I think that exact same thing, write a 100 word paragraph, let's create a chat, let's go to a 1 and do this, okay, so it's going to do some reasoning and has a much higher likelihood, not a guarantee, that it will hit that 100 limit.
09:23 So one thing you'll notice is because this reasoning, for one second, it had an analyzing thing right here. It's actually doing Python to count the number of words while it's coming up with it.
09:35 So if we go like this, assuming it didn't take special characters as words, it is spot on as 100 words.
09:43 So that's the difference between chain of thought versus non-chain of thought. Non-chain of thought is prey that it works. Chain of thought is increased to like the hood by 3, 4, 5, 6, X, they will work, so very useful to be aware of there.
09:59 Now chain of thought example, I just showed you one, so we can kind of glance over this one. A farmer has 17 cows, all but 5, 12 and all but 9 on their left, just a standard riddle.
10:10 If you gave this riddle to a 40, it might struggle, honestly we'll struggle, we don't even have to test it out.
10:15 Whereas a CO2 model will do the farmer starts with 17 cows all by nine die, meaning the nine cows are still alive, yada yada yada, and then it figures it out.
10:27 So that's chain of thought. Move it on. Another one you probably have not heard of as much is called scaffolding.
10:34 So scaffolding is a prompt design where you break a complex task into smaller or manageable steps so you can prompt the model through each of those steps.
10:44 So, a really good example of this that I've done way before in the past had this random famous author reach out to me on Fiverr for at the time something very experimental.
10:56 At the time we only had a GPT-4 and it was a very small context window of I believe 8000 tokens, which is not very much what you're trying to output, which I think the output was the max of 2000 something tokens per prompt.
11:11 So that exercised at the time, although experimental, I was able to figure out using Python, but he just really didn't like the copy.
11:20 So he wasn't very happy with the end product despite us figuring out the engineering side. All that's say, if you were to create a book using GP40, the limited context window, you'd have a few different issues.
11:32 One, you can't output a whole chapter at a time, at least for an actual proper book, right? it. True, when it comes to developing that book, you first want to actually need to create the outline of the chapters.
11:45 And then once you're happy with that, then you start writing the intro of that chapter. And then you could take that intro and you throw it in as context for the next part of the chapter.
11:58 Let's say the middle part. And then you keep doing that. And what you notice is, as you go from chapter one to chapter two to chapter three, even And if you're respecting the outline of the book, there's going to be a lot of overlapping phrasing, formulaic phrasing, and structure of the actual chapters
12:17 . The fluidity of a book goes away, at least in that specific use case at that specific time. But scaffolding was the only way we could make it happen, meaning how do we break this process down of having a book with a table of contents with different chapters, with different content in those chapters
12:34 with different varying length requirements of those chapters in sequence and the need for one chapter to ideally build on top of and segue to the next one.
12:43 So this is an example where scaffolding is super helpful and even though we have much larger context window much more powerful models scaffolding is still awesome because if we take that exact example that I gave you right now that book one and we brought it to today today then And you'd be basically
13:03 creating an outline of this book. And from that book, you would now be able to probably write one whole chapter at a time, especially if you use something like O3 mini, which has a very large output token window, not input token.
13:17 So input token, I think, is around 200,000 at 200,000, yep. And then output tokens is how many tokens are going to actually generate as a response.
13:27 Most morals still aren't that great at that. Even the clawed, I think it's around from 40 to 60,000. Gemini 2.5 Pro, as of this recording, is around 64,000.
13:38 O3 Mini is 100,000, which is actually a big deal, meaning you can write 10, 15, 20 pages with the right series of prompts.
13:46 So this is what scaliting looks like. You're basically doing construction, and you're providing structure and step-by-step approach. So how do you scaffolding multi-prompt scaffolding where it could look like this, prompt one, give me an outline for an essay about climate change, prompt dose.
14:04 Now we want to write the first section about impacts on agriculture, prompt dress, now write the section about impacts on oceans.
14:13 Then we have single prompt scaffolding where you throw it all at once. You say first, list key points about climate change.
14:19 Then for each point provided detailed explanation of evidence, Finally, conclude with recommendations for addressing climate change. So in this case, this is actually how you'd ideally, right now, load this into a reason you want to like a one or three or three point seven on it or two point five pro
14:37 . You would give it this as a blob of text and if you have any information to give it, you would dump it down below structurally.
14:43 So this is going to be super useful for you. When and when, when and why to use scaffolding. So use scaffolding when it's too complex to expect a good answer in one shot.
14:55 And it is ideal for long-form content, multi-part analysis, content that benefits from a clear structure or sequence. And then the benefits overall is obviously reduces wandering, reduces the likelihood of hallucination, why, similar to chain of thought.
15:10 You kind of making bite-sized tasks for all these sub-proms. So, the likelihood that they can accomplish this specific myopic task is better, versus having a really large prompt, where the likelihood of not succeeding in one area, or with the loot back to our first lesson, which was attention, giving
15:30 the proper attention to every part of the prompt, becomes too complex or way too hard to do consistently. If you want another analogy, imagine you are a parent of five kids, do you want to give equal love to everything?
15:44 Yes. Do you want to give the equal attention to everyone? Yes. Does that happen in practice usually, 899.9% of the time?
15:52 No. Not out of maliciousness, but just time in the day, 24 hours you have yourself just doesn't happen. So same thing with the prompts, call them up for now, even with the great context windows, there are parts that kind of get fallen out or less attended to or less attention is paid to those requirements
16:11 or instructions so suddenly this can help a lot with those tasks. All right so in terms of an example so in a single prompt approach we could say give me a swap analysis of company X and then scaffolded approach again would be 1 list the strengths of company X 2 list the weaknesses.
16:32 Three, list them for opportunities. Four, now list the threats. Five, combine the above into a full-swa analysis report. Notice this one.
16:42 So this one is not just carrying out indirect and independent tasks. All of them are coming together. Everything is converging and merging and being additive to the final output.
16:55 So that should be super helpful there. Next, utilizing context and examples effectively. So models operate in the here and now of the prompt, so for multi-turn scaffolding, remind the model of key points.
17:09 So an example is continuing the essay on climate change impacts, next discuss on oceans or sorry impact on oceans and for few shot examples, you want to do some like pattern matching.
17:21 So when it comes to multi-turn, you're just adding more and more components to your initial prompt set, which we saw on the last slide before.
17:29 Last thing is order matters, so if we go back to the prior slide, see how this was like strengths, then weaknesses, then opportunities, and threats.
17:39 If you think of it from a strategic standpoint, knowing that this is all predictive, if you talk about the strengths, the likelihood that you can get now the next chain prompt to look at the strengths and use that to help contrast it with weaknesses relative to those strengths is higher.
17:58 Now, if you do the strengths and the weaknesses, the likelihood they can use both of those inputs to now look for opportunities, because our usually opportunities lie between a strength and weakness is higher.
18:09 So the structure in which you sequence these prompts is super helpful slash impactful to the results you're going to get.
18:19 Now utilizing context, the examples effectively went over this and then an example here of tree of thought. So this is now we're moving on to a very different nuanced, more indie type of prompting.
18:34 So it's called tree of thought prompting. And this is probably one of the least type of prompts that I see in the wild, not because it doesn't have merit, but most people don't know how to use it, when to use it, and where to use it.
18:46 So what is it? So instead of COT, chain of thought, this is TOT, tree of thoughts, is a prompting approach where the model explores many possible solutions by branching out like a tree.
19:00 So if we took a look, take a look rather at the left hand side, chain of thought prompting, you have uno dos tris, like we saw before, prompt one, two and three, whereas prompt, tree of thought prompting is one, and then you can make a fork in the road either two A or two B.
19:17 If it's 2A it might go to 3A. If it goes to 2B, maybe you can hop to 2A and go to 3A or 3B.
19:24 There is some variability in the path of thinking. So the analogy I give is it's like a brainstorming session where you don't stop at the first idea.
19:35 Whereas chain of thought is sequential. We're going in a pretty much a linear path. Tree of thought lets you go in the unknown on your path.
19:43 So, carrying on and pulling on that thread. Tree of Thought is very powerful for creative problems, complex puzzles or problems, strategic planning, creative writing, and the benefits is, you'll notice that tree of Thought is very applicable to something like deep research, or deep research for any of
20:03 the vendors, whether it be Gemini, whether it be Prophexity, or ChatGbT's OpenAI. It's not necessarily linear thinking, right? So you send a request to deep research.
20:14 It goes in peruses the entire web or at least it's index of the web That's more of a fine print thing Comes back with the results.
20:23 Those results it now synthesizes now It had a task to go and look for certain Sources and then audit those sources and now you when you bring them back You have to marry all those conclusions and now run it through the filter of what the person even asked for right?
20:38 What was the main LM query? So we have reasoning. We have some different dimensionality of looking at sources and other sources and breaking them down and reasoning through them.
20:48 All of this resembles what Tree of Thought looks like. So in client projects, very, very rarely have we ever had to resort to something like this.
20:59 But if this helps you frame in mind what deep research looks like, it's a good example of this. But obviously the trade off is it's a lot more involved.
21:08 This is where like someone like me would be usually hired to come in and take care of it because When you run this prediction stuff like if you go back to this tree model I come from the world of machine learning in the eye.
21:21 So five six years ago. We used to run machine learning models called random forest tree-based problems and This is a very familiar diagram to me and this is a series of probabilities and as you go down the tree as we would say things become more over indexed.
21:40 You're now running a prediction on a prediction, on a prediction, and you're using so many derivatives of the single prediction that you have to keep track of what's happening.
21:49 So even if we take my analogy of these models not being magic but being predictive, when you use tree of thought you have this like underlying conduit of math happening underneath the query that you're sending, there's a lot happening.
22:05 So when things go wrong, it's hard to trace which part of the tree is run, which branch is run? All right.
22:15 So example here, so you have a three gallon and a five gallon jug. How can you measure exactly four gallons of water?
22:24 So in this case, generation phase generate three possible solution strategies, strategy eight, Fill a 5-gallon jug, pour into a 3-gallon jug, strategy B, fill a 3-gallon jug twice, pour into a 5-gallon jug, and then strategy C, fill an empty jugs in an alternating pattern.
22:43 So, other than understanding and obsessing about water jugs, what can we learn from that one snippet? Ten front would be like, fill the jug, now fill the jug with the optimal amount of quantity.
22:57 So is it five gallon, three gallon, seven gallon, et cetera. In that chain prompting methodology and thought pattern, you wouldn't necessarily go through and be like, okay, what do we just fill empty jugs in an alternating pattern?
23:12 There's this like anomalous thought, this out of the context of the meaning thought or out of the box thought that is taken into consideration before you go into the evaluation stage where maybe you simulate each strategy step by step, a try to converge to what is the best strategy.
23:30 So this is a more, again, indie, creative strategy. And this is not something I would use for an AI-cold email automation.
23:40 It's not something you want to really employ there. You want some level of deterministic behavior. This is the definition of chaotic behavior.
23:49 But from chaotic behavior, you can yield very nuanced results. And in some lines of business or industries, this could be cool.
23:56 And last thing here, a little note. The model explored a space of solutions rather than one reasoning line. So that's helpful to know as well.
24:06 Alright, now we're moving on to a different tier of prompting, more familiar. It's something where you probably have done it before, or you've seen it before, but you just didn't have the words to articulate the definition, which is self-reflection and critique prompting.
24:22 What it is, the TLDR, is a prompting technique where the model is asked to review and critique its own output before finalizing an answer.
24:32 So you can see it's also derivative of these reasoning models. So this concept existing existed rather before reasoning models existed.
24:40 So traditional output is question, answer, input, output, So, self-reflection output was question, initial answer, then critique of said answer, and improve the answer.
24:53 So, perfect use case again, that's derivative of chain prompting, where you have input one, output one, input two, output two, that takes a consideration, input one, input two, and output two.
25:07 But it really reflects on output two and how can make it better. So, it's thematically in the same horizon. How to use it, an example would be, do you spot any mistakes or areas to improve your answer?
25:20 So please correct them. So in a chain prompt example, you could run the chain prompt linearly, except you have the one of them being like, look at the copy of this article or this blog or this email.
25:32 And just ask yourself, if you were in the shoes of prospect, then maybe it would be cool to add like the dynamic variable, James Smith from electric power company, blah blah blah.
25:43 How would you want this email to read to you? Based on your reflection on that, optimize the email, knowing that he likes insert other variable you collected on a lead form or something like that.
25:55 So this is where you could use that in practice when it comes to chain prompting, and then you should be good to implement it from there.
26:02 there. Single prompt and multiple steps. So another example is first answer the question. Then analyze the answer to see if it could be wrong or improved.
26:12 And if the answer needs revision, provide a revised answer. And then number, Tress here, Advanced Reflection Approach, which is the model produces multiple penitems, each got it by a critique of the previous attempt.
26:25 So let's take our tree example, except we don't have different paths necessarily of thinking we have different possible outputs so good examples let's say when I make a YouTube video right now I want to get the SEO and the keywords proper I might say to 4.5 GPT 4.5 make me 10 different titles that are
26:47 along the lines of this and I'll insert like my description of what I did the video about so you'll get 10 different outputs the next possible step right in this case using software flexion is all right cool if you were a member of my ICP them looking for which is kind of business owners entrepreneurs
27:04 blah blah blah regenerate some optimized titles for that audience and provide rationale for why this would or wouldn't be a hit with that audience right so I'm adding that next here to the output where it's not just now another set of improved enhanced or changed titles.
27:26 It's now improved and has changed. Plus it's being tailored to the audience with some rationale and reflection as to why or why not I might want to use that title for my video.
27:37 So that's software flexion. I actually like it quite a bit. In this case, I think we went over this for the most part.
27:46 This should say software flexion. When to use it models often recognize a mistake when asked to reflect. Even if they make the mistake in the first pass, ideal for complex reasoning tasks where errors are likely, creative writing tasks that eat refinement, and scenarios where incorrect answers could 
28:04 be costly. One important thing here, I'm glad I put it here, which is in self-reflection, similar to human beings, sometimes we can overthink.
28:16 When we overthink, sometimes we make mistakes. really good example is have you ever studied for a test so much and you over study to the point where now you're so smart and nuanced on the topic that when you look at the multiple choice questions you can find a really smart way to rationalize why the 
28:35 answer is both a and d even though there's no all the above and there's no pick a and d can you can you get to that point where you're overthinking.
28:44 Yes. In this case, you can also pose the question, you know, if you were in Bob Smith's or Sarah James spot, how would you look at this email?
28:54 And you could kind of push it a bit too far where what if you get like an anomalous weird lead form and automatically sends a cold email draft or a cold email directly using verbiage that you would never use because that lead was just so super nuanced and weird and anomalous, you might have a perfect
29:15 answer or a perfect email that was super professional, clear cut, concise, coldly calculating, and now you've ruined it and you've made it kind of too playful or way outside your brand guidelines because it went the extra step to refine and reflect on it.
29:31 So always a grain of salt when you're implementing AI at scale especially when it comes to prompt engineering and doing these little fancy man she different types of prompt engineering.
29:42 Alright, and then a few more examples. I think there's, yeah, so we have two more examples here. Math problem, provide a step-by-step solution to a train leave city A.
29:54 And then the follow-up was check your solution for any errors and explain upon reviewing. I realized I assumed the trains travel at the same time which was incorrect.
30:03 The mistake is in step three, et cetera. And then the model provides a correct solution. a creative running example, single prompt, write a short poll about technology, then critique its style and tone and produce a final improved poem.
30:16 The numero uno model writes initial poem, two model critiques, the tone is a bit dry, could be more playful, three, model provides a revised poem with a more playful tone.
30:27 So that should give you a good example of how you can use self-reflection pretty safely and pretty effectively to do this.
30:36 Advanced self-reflection, a really good example is like AI and Thropic, how they trained it. After an answer, it typically checks as a part of its guardrails.
30:46 Does this follow the principles of honesty and harmlessness? This is in their documentation of how their AI outputs when it outputs a response.
30:55 So there's probably some form of raw response from the Cloud2.7 on API, and then you have have some like wrapper functions around it to make sure that it doesn't contain certain times of speech, or, and, or, in the actual UI of Claudii, maybe there's like another system prompt that's layered on top that
31:14 makes sure this doesn't see the light of day. If it's something that's really negative or is harmful, for example, right?
31:21 Excuse me. So, when it comes to self-consistency, generate multiple answers and see if they agree. And the example here is, run the prompt several times with slight randomness and choose the answer that the most outputs converge on.
31:36 And the next here is multiple perspectives, even from the same model, lead to better answers. Typically, I'm going to say comma, typically.
31:44 But this needs to be tested, this needs to be clarified, and most importantly, it needs to just double check whether or not this is even necessary.
31:51 Based on the types of models we have today, which, depending on the time you watch this video, we could have reasoning models that can think for like 20 minutes or do 20 minutes worth of thinking in seconds.
32:05 Now when it comes to context, window, and sensitivity, I'll leave this for the next part so I can have yet another one of these and come back to you.